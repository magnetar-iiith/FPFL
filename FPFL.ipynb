{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "FPFL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbFzPQompLUv"
      },
      "source": [
        "### import #########################\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "import tensorflow_privacy\n",
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "import tensorflow.keras.backend as kb\n",
        "import h5py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDugV1O1pLU0"
      },
      "source": [
        "#### Adult Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqjjDq_dpLU0"
      },
      "source": [
        "############ ADULT ################\n",
        "# df = pd.read_csv('adult_prot.csv')\n",
        "# df_ap = df.loc[(df['103']== 1) & (df['104'] == 1)]\n",
        "# df=df.append([df_ap]*1)\n",
        "# df = df[:-611]\n",
        "# data = df.values[:,:-1]\n",
        "# prot = df.values[:,-1]\n",
        "# print(data.shape, prot.shape)\n",
        "# feat_size = data.shape[1]-1\n",
        "\n",
        "# df = pd.read_csv('compass_new.csv')\n",
        "# data = df.values[:,:-1]\n",
        "# prot = df.values[:, -1]\n",
        "# feat_size = data.shape[1]-1\n",
        "# print(sum(prot), sum(data[:,-1]))\n",
        "# print(feat_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1xmK12lpLU1"
      },
      "source": [
        "#### Bank Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmoJU5ZypLU2"
      },
      "source": [
        "# ########## BANK #####################\n",
        "# df = pd.read_csv('bank_prot.csv')\n",
        "# indx = df[df['63'] == 0].index.values[:1188]\n",
        "# update_df = df.drop(indx)\n",
        "\n",
        "# data = update_df.values[:,:-1]\n",
        "# prot = update_df.values[:,-1]\n",
        "# print(sum(prot), data.shape)\n",
        "# feat_size = data.shape[1] - 1\n",
        "# print(sum(1 - data[:,-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PExBD6XrpLU3"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFYakabKpLU4"
      },
      "source": [
        "# fed_ml params\n",
        "splits = 5\n",
        "samples_per_network = int(0.8*len(data)/(splits))\n",
        "models = []\n",
        "data_split = []\n",
        "\n",
        "# architecture\n",
        "# hidden_size = [1000, 500] ## adult\n",
        "hidden_size = [500, 100]  ## bank\n",
        "\n",
        "num_classes = 2\n",
        "input_size = [samples_per_network, data.shape[1]-1]\n",
        "\n",
        "# optimizer and loss\n",
        "learning_rate = 0.25 ## for phase 2\n",
        "rho = 10\n",
        "batch_size = 500\n",
        "# num_epochs = 100 ## phase 1\n",
        "num_epochs = 20 ## phase 2\n",
        "\n",
        "l2_norm_clip = 1.5\n",
        "noise_multiplier = 1.3  ### sigma\n",
        "num_microbatches = 250\n",
        "\n",
        "# if batch_size % num_microbatches != 0:\n",
        "#   raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "# fairness params\n",
        "epsilon = 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGLSWE06pLU4"
      },
      "source": [
        "#### MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCrgS0e0pLU5"
      },
      "source": [
        "def model_init(flag):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(hidden_size[0],\n",
        "                               activation='relu',\n",
        "                               input_shape=(feat_size,)),\n",
        "        tf.keras.layers.Dense(hidden_size[1], activation='relu'),\n",
        "        tf.keras.layers.Dense(num_classes)\n",
        "    ])\n",
        "    \n",
        "    if flag is None:\n",
        "        ## init weights from the fair model\n",
        "        f = h5py.File('path/to/saved_model', 'r')\n",
        "        bias = []\n",
        "        kernel = []\n",
        "        for i in list(f.keys()):\n",
        "            bias.append(f[i][i]['bias:0'].value)\n",
        "            kernel.append(f[i][i]['kernel:0'].value)\n",
        "        ctr = 0\n",
        "        for layer in model.layers:\n",
        "            if ctr == 2:\n",
        "                layer.set_weights([kernel[ctr][:,:2], bias[ctr][:2]])\n",
        "            else:\n",
        "                layer.set_weights([kernel[ctr], bias[ctr]])\n",
        "            ctr += 1\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suMWx4DspLU7"
      },
      "source": [
        "#### Training function for fair-SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9YoAWa3pLU7"
      },
      "source": [
        "def train_per_model(train_x, train_y, test_x, test_y, train=True, wts=None):\n",
        "    \n",
        "    model = model_init(1)\n",
        "    \n",
        "    if wts != None:\n",
        "        model.set_weights(wts)\n",
        "        \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "    def fair_loss(y_actual, rounded):\n",
        "        \n",
        "        y_true = y_actual[:,:2]\n",
        "        output = rounded[:,:2]\n",
        "        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "        \n",
        "        loss1 = loss(y_true, output)\n",
        "        prot_data = y_actual[:,2:]\n",
        "        rounded = output\n",
        "        c0 = kb.sum((rounded[:,1] * (prot_data[:,0])))/ (kb.sum(prot_data[:,0]))\n",
        "        c1 = kb.sum((rounded[:,1]* prot_data[:,1]))/(kb.sum(prot_data[:,1]))\n",
        "        loss2 = kb.maximum(kb.abs(c0 - c1) - epsilon, 0.0)\n",
        "        return loss1 + rho * loss2\n",
        "    \n",
        "    def DemP(y_true, y_pred):\n",
        "        y_pred = kb.softmax(y_pred[:,:2])\n",
        "        prot_data = y_true[:,2:]\n",
        "        c0 = kb.sum((y_pred[:,1] * (prot_data[:,0])))/ (kb.sum(prot_data[:,0]))\n",
        "        c1 = kb.sum((y_pred[:,1]* prot_data[:,1]))/(kb.sum(prot_data[:,1]))\n",
        "        loss2 = kb.abs(c0 - c1)\n",
        "        return loss2\n",
        "\n",
        "    \n",
        "    if train:\n",
        "        model.compile(optimizer=optimizer, loss= fair_loss, metrics=['accuracy', DemP])\n",
        "        es = tf.keras.callbacks.EarlyStopping(monitor='val_acc', mode='max', min_delta=0.01, patience=200)\n",
        "        model.fit(train_x, train_y,\n",
        "                  epochs=num_epochs,\n",
        "                  validation_data=(test_x, test_y),\n",
        "                  batch_size=batch_size, callbacks=[es])\n",
        "\n",
        "    else:\n",
        "        model.set_weights(wts)\n",
        "        model.compile(optimizer=optimizer, loss=fair_loss, metrics=['accuracy', DemP])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWCQvP3hpLU8"
      },
      "source": [
        "#### Training function for DP-SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmYgE3zIpLU9"
      },
      "source": [
        "model_target = model_init(None)\n",
        "def train_per_model(train_x, train_y, test_x, test_y, train=True, wts=None):\n",
        "    \n",
        "    model = model_init(0)    \n",
        "    \n",
        "    optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
        "        l2_norm_clip=l2_norm_clip,\n",
        "        noise_multiplier=noise_multiplier,\n",
        "        num_microbatches=num_microbatches,\n",
        "        learning_rate=learning_rate)\n",
        "    \n",
        "    if wts != None:\n",
        "        model.set_weights(wts)\n",
        "\n",
        "    \n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "    \n",
        "    if train:\n",
        "        model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "        es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.1, patience=10)\n",
        "        model.fit(train_x, kb.eval(kb.softmax(model_target(train_x))),\n",
        "                  epochs=10,\n",
        "                  validation_data=(test_x, test_y),\n",
        "                  batch_size=batch_size, callbacks=[es])\n",
        "    else:\n",
        "        model.set_weights(wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvQ7_bntpLU-"
      },
      "source": [
        "### Train and test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmVx_cexpLU-"
      },
      "source": [
        "test_ix = np.random.randint(0, len(data), int(0.2*len(data)))\n",
        "test_data = data[test_ix, :-1]\n",
        "test_prot =  tf.keras.utils.to_categorical(prot[test_ix, np.newaxis], num_classes=num_classes)\n",
        "test_labels = tf.keras.utils.to_categorical(data[test_ix, -1], num_classes=num_classes)\n",
        "test_labels = np.append(test_labels, test_prot, 1)\n",
        "\n",
        "split_datasets = []\n",
        "split_labels = []\n",
        "for i in range(splits):\n",
        "    t_ix = np.random.randint(0, len(data), samples_per_network)\n",
        "    split_datasets.append(data[t_ix, :-1])\n",
        "    split_prot = tf.keras.utils.to_categorical(prot[t_ix, np.newaxis], num_classes=num_classes)\n",
        "    split_labels.append(np.append(tf.keras.utils.to_categorical(data[t_ix, -1], num_classes=num_classes), split_prot, 1))\n",
        "    \n",
        "trained_wts = None\n",
        "for itr in range(int(num_epochs/10)):\n",
        "    for i in range(splits):\n",
        "        model = train_per_model(split_datasets[i], split_labels[i], test_data, test_labels[:,:2], wts = trained_wts)\n",
        "        if i == 0:\n",
        "            layerwise_weights = []\n",
        "            for l in model.get_weights():\n",
        "                layerwise_weights.append(l/splits)\n",
        "        else:\n",
        "            ctr = 0\n",
        "            for l in model.get_weights():\n",
        "                layerwise_weights[ctr] += l/splits\n",
        "                ctr += 1\n",
        "    trained_wts = layerwise_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj2jMgOZpLU-"
      },
      "source": [
        "### Moments accountant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWtJnAKHpLU_"
      },
      "source": [
        "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=samples_per_network,\n",
        "                                              batch_size=batch_size,\n",
        "                                              noise_multiplier=noise_multiplier,\n",
        "                                              epochs=num_epochs,\n",
        "                                              delta=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}